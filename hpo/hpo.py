# -*- coding: utf-8 -*-
"""HPO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sFV5wSyxqKG4ZLP76tj8p6RrVbeK1waq
"""

#!pip install optuna
#!pip install pickle5
import numpy as np
import torch
import os
import torch 
from torch import optim #PyTorch additionals and training optimizer
import torch.nn as nn
import torch.nn.functional as F #PyTorch library providing a lot of pre-specified functions
import os
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
from torch import optim
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import optuna 
import pickle5 as pickle

"""# Dataprep"""

class PytorchDataset(Dataset):
    
    def __init__(self, path, modality):
        """
        Purpose: 
            Load pickle object and save only specified data. 
        """
        dat = pickle.load(open("/content/drive/MyDrive/thesis/data/bci12.pickle", "rb")) #Open pickle
        key = "labels_"+modality #Determining right dict key
        self.labels = dat[key] #Saving labels
        self.dat = dat[modality] #Saving only modality of interest
        self.modality = modality

        #Determining the longest window for later use
        lengths = []
        for i in self.dat:
            lengths.append(len(i))
        longest_window = max(lengths)


    def __len__(self):
        """
        Purpose: 
            Obtain the length of the data
        """
        return len(self.dat)


    def __getitem__(self, idx):
        """
        Purpose:
            Iterater to select windows
        """
        windows = self.dat[idx]
        labels = self.labels[idx]

        return windows, labels
    

    def __PaddingLength__(self):
        """
        Purpose:
            Determine padding length
        """
        lengths = []
        for i in self.dat:
            lengths.append(i.shape[0])
        
        return max(lengths)

    def __ObtainModality__(self):
        """
        Purpose:
            Print modality
        """
        
        return self.modality


#Batch transformation class
class BatchTransformation():
    def __call__(self, batch):
        """
        Purpose:   
            Transformation of windows per batch (padding & normalizing labels & transposing)
        """

        padding_length = self.padding_length
        modality = self.modality

        #PADDING
        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True) #Sort batch in descending
        sequences = [x[0] for x in sorted_batch] #Get ordered windows
        
        sequences.append(torch.ones(padding_length,1)) #Temporary add window of desired padding length
        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value = 0) #Pad
        sequences_padded = sequences_padded[0:len(batch)] #Remove the added window

        #Obtaining Sorted labels and standardizing
        labels = torch.tensor([x[1] for x in sorted_batch]) #Get ordered windows
        labels = (labels - 1)/ 20
        
        #TRANSPOSE BATCH 
        sequences_padded = torch.transpose(sequences_padded, 1, 2)

        """
        #EEG prep
        if modality == "EEG": 
            sequences_padded = sequences_padded.unsqueeze(1)
        """    

        return sequences_padded, labels
    
    def transfer(self):
        """
        Purpose:
            Transfering the earlier obtained padding length to the BatchTransformation class such it can be used
            in the __call__ function.
        """
        BatchTransformation.padding_length = self[0]
        BatchTransformation.modality = self[1]

"""# Networks"""

"""
@Author: Bart-Jan Boverhof
@Last Modified by: Bart-Jan Boverhof
@Description Single-modular deep neural network design for the EEG-modality.
"""

################### 0. Prerequisites ###################
#Loading packages
import torch #PyTorch deep-learning library
from torch import optim #PyTorch additionals and training optimizer
import torch.nn.functional as F #PyTorch library providing a lot of pre-specified functions
import torch.nn as nn
from torchsummary import summary

################### EEG Net ###################
class EEGNet(nn.Module):
    def __init__(self, tensor_length, out_features, drop):
        super(EEGNet, self).__init__()

        self.drop = drop
        self.tensor_length = tensor_length
        foo = int(tensor_length /3)
        foo = int(foo /3)         
        foo = int(foo /3)
        foo = int(foo /3)
        dense_input = 200*foo

        #Convolutional layers
        self.conv1 = nn.Conv1d(in_channels = 4, out_channels = 25, kernel_size = 3, padding=1)
        self.conv2 = nn.Conv1d(in_channels = 25, out_channels = 50, kernel_size = 3, padding=1)
        self.conv3 = nn.Conv1d(in_channels = 50, out_channels = 100, kernel_size = 3, padding=1)
        self.conv4 = nn.Conv1d(in_channels = 100, out_channels = 200, kernel_size = 3, padding=1)

        #Max pooling layer (3x1)
        self.pool = nn.MaxPool1d(kernel_size = 3, stride = 3) 

        #Batch normalization
        self.batch1 = nn.BatchNorm1d(num_features = 25)
        self.batch2 = nn.BatchNorm1d(num_features = 50)
        self.batch3 = nn.BatchNorm1d(num_features = 100)
        self.batch4 = nn.BatchNorm1d(num_features = 200)

        #Dense layer
        self.dense1 = nn.Linear(dense_input, out_features) 
        self.dense2 = nn.Linear(out_features, 1) 

        #Dropout layer
        self.dropout = nn.Dropout(drop)

        
    def forward(self, x): 
        x = self.pool(F.elu(self.batch1(self.conv1(x)))) #First block
        x = self.pool(F.elu(self.batch2(self.conv2(x)))) #Second block
        x = self.pool(F.elu(self.batch3(self.conv3(x)))) #Third block
        x = self.pool(F.elu(self.batch4(self.conv4(x)))) #Fourth block
        
        x = x.view(-1, x.shape[1]* x.shape[2]) #Flatten
        x = self.dropout(x)
        x = F.relu(self.dense1(x))
        x = self.dense2(x)

        return x

#Display network
#eeg_net = EegNet()
#summary(eeg_net, (1000,1))

################### PPG Net ###################
class PPGNet(nn.Module):

    def __init__(self, filters = 32, hidden_dim = 64, n_layers =2, drop = 0.25):
        super(PPGNet, self).__init__()
        
        self.ip_dim = filters
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.drop = drop

        #Convolutional layers
        self.conv1 = nn.Conv1d(in_channels =  1, out_channels = filters, kernel_size = 3, padding=1)
        self.conv2 = nn.Conv1d(in_channels = filters, out_channels = filters, kernel_size = 3, padding=1)

        #Max pooling layer (4)
        self.pool = nn.MaxPool1d(kernel_size = 4, stride = 4) 

        #Batch normalization
        self.batch1 = nn.BatchNorm1d(num_features = filters)
        self.batch2 = nn.BatchNorm1d(num_features = filters)
        self.batch3 = nn.BatchNorm1d(num_features = int(hidden_dim/4))

        #Dense & classification layer 
        self.fc1 = nn.Linear(hidden_dim, int(hidden_dim/4))
        self.fc2 = nn.Linear(int(hidden_dim/4), 1)


        #LSTM layer
        self.lstm = nn.LSTM(input_size = filters, 
                                hidden_size = hidden_dim, 
                                num_layers = n_layers, 
                                dropout = drop,
                                batch_first= True)
    
    def InitHiddenstate(self, batch_size):
           
        weight = next(self.parameters()).data
        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),
                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())
        
        return hidden

    def forward(self, x, hidden): 

        batch_size = x.shape[0]
        #Convolutional block
        x = self.pool(F.relu(self.batch1(self.conv1(x)))) #First block
        x = self.pool(F.relu(self.batch2(self.conv2(x)))) #Second block
        
        #LSTM-block
        x = torch.transpose(x, 1, 2)
        out, hidden = self.lstm(x, hidden)
        out = out.contiguous().view(-1, self.hidden_dim)

        #Prediction block
        out = F.relu(self.batch3(self.fc1(out)))
        out = self.fc2(out)

        out = out.view(batch_size, -1)
        out = out[:,-1]

        return out, hidden

#Display network
#model = PpgNet()



################### GSR Net ###################
class GSRNet(nn.Module):

    def __init__(self, filters = 32, hidden_dim = 64, n_layers =2, drop = 0.25):
        super(GSRNet, self).__init__()
        
        self.ip_dim = filters
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.drop = drop

        #Convolutional layers
        self.conv1 = nn.Conv1d(in_channels =  1, out_channels = filters, kernel_size = 3, padding=1)
        self.conv2 = nn.Conv1d(in_channels = filters, out_channels = filters, kernel_size = 3, padding=1)

        #Max pooling layer (4)
        self.pool = nn.MaxPool1d(kernel_size = 4, stride = 4) 

        #Batch normalization
        self.batch1 = nn.BatchNorm1d(num_features = filters)
        self.batch2 = nn.BatchNorm1d(num_features = filters)
        self.batch3 = nn.BatchNorm1d(num_features = int(hidden_dim/4))

        #Dense & classification layer 
        self.fc1 = nn.Linear(hidden_dim, int(hidden_dim/4))
        self.fc2 = nn.Linear(int(hidden_dim/4), 1)


        #LSTM layer
        self.lstm = nn.LSTM(input_size = filters, 
                                hidden_size = hidden_dim, 
                                num_layers = n_layers, 
                                dropout = drop,
                                batch_first= True)
    
    def InitHiddenstate(self, batch_size):
           
        weight = next(self.parameters()).data
        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),
                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())
        
        return hidden

    def forward(self, x, hidden): 

        batch_size = x.shape[0]
        #Convolutional block
        x = self.pool(F.relu(self.batch1(self.conv1(x)))) #First block
        x = self.pool(F.relu(self.batch2(self.conv2(x)))) #Second block
        
        #LSTM-block
        x = torch.transpose(x, 1, 2)
        out, hidden = self.lstm(x, hidden)
        out = out.contiguous().view(-1, self.hidden_dim)

        #Prediction block
        out = F.relu(self.batch3(self.fc1(out)))
        out = self.fc2(out)

        out = out.view(batch_size, -1)
        out = out[:,-1]

        return out, hidden
#Display network
#GsrNet()



################### Multi-modular Net ###################



################### Test Net ###################
class Network(nn.Module):
    def __init__(self):
        super().__init__()
        
        # Inputs to hidden layer linear transformation
        self.hidden = nn.Linear(784, 256)
        # Output layer, 10 units - one for each digit
        self.output = nn.Linear(256, 10)
        
        # Define sigmoid activation and softmax output 
        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
        # Pass the input tensor through each of our operations
        x = self.hidden(x)
        x = self.sigmoid(x)
        x = self.output(x)
        x = self.softmax(x)
        
        return x

#Network()

"""# Train"""

#!/usr/bin/env python
"""
@Author: Bart-Jan Boverhof
@Last Modified by: Bart-Jan Boverhof
@Description Loading the data and training all networks.
"""


###########################################################################################
###################################### 0. Prerequisites ###################################
###########################################################################################

def objective(trial):
    
    ###########################################################################################
    ########################## 1. Create PyTorch dataset & Loader(s) ##########################
    ###########################################################################################
    #Create PyTorch dataset definition class
    #path = "pipeline/prepared_data/"+participant+"/data.pickle"
    pydata =  PytorchDataset(path = "path", modality = modality)

    padding_length = PytorchDataset.__PaddingLength__(pydata) #Determining the longest window for later use
    BatchTransformation.transfer([padding_length, modality]) #Transfer max padding length & modality vars to BatchTransfor class
               
               
    #Making splits
    validation_split = .1

    ################
    ## TEST SPLIT ##
    ################    
    indices = list(range(len(pydata.dat))) #Create list of indices
    np.random.shuffle(indices) #Shuffle indices.

    train_valid_split = int(np.floor(validation_split * len(indices))) #Calculate number of windows to use for val/train
    train_indices, val_indices = indices[train_valid_split:], indices[:train_valid_split] #Splitted indices.

    #Defining samplers
    train_sampler = SubsetRandomSampler(train_indices) #Train sampler
    valid_sampler = SubsetRandomSampler(val_indices) #Validation sampler
    
    #Defining loaders
    validloader = torch.utils.data.DataLoader(pydata, #Validation loader
                                            batch_size = batch_size, 
                                            shuffle = False,
                                            drop_last= False,
                                            sampler = valid_sampler,                                          
                                            collate_fn = BatchTransformation())

    trainloader = torch.utils.data.DataLoader(pydata, #Training loader
                                            batch_size = batch_size, 
                                            shuffle = False,
                                            drop_last= False,
                                            sampler = train_sampler,
                                            collate_fn = BatchTransformation())





    ###########################################################################################
    ################################## 2. Defining model ######################################
    ###########################################################################################
    #Defining network  

    #Determine max out_features
    max_out = int(padding_length /3)
    max_out = int(max_out /3)         
    max_out = int(max_out /3)
    max_out = int(max_out /3)
    max_out = 200*max_out
    
    out_features = trial.suggest_int("n_units", 10, max_out)
    drop = trial.suggest_float('dropout_rate', 0.2, 0.5)

    if modality == "PPG":
        model = PPGNet(drop = drop)
    elif modality == "GSR":
        model = GSRNet(drop = drop)
    elif modality == "EEG":
        model = EEGNet(tensor_length = padding_length, drop = drop, out_features = out_features)
    
    #Hyperparams
    #Optimizer and learning rate
    lr = trial.suggest_float("lr", 1e-5, 1e-1, log = True)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()


    if train_on_gpu:
        model.cuda()

    ###########################################################################################
    ########################## 3. Training & Validation loop ##################################
    ###########################################################################################
    
    if trainortest == "train":
        #Prerequisites
        clip = 5
        train_list = []
        valid_list = []
        valid_loss_min = np.Inf

        ######################
        ### PPG & GSR LOOP ###
        ######################
        if modality == "PPG" or modality == "GSR": #If the network includes an LSTM layer (holds for PPG & GSR) 
            for epoch in range(1, epochs+1):
                    
                train_loss = 0.0
                valid_loss = 0.0

                #####################
                ### Training loop ###
                #####################
                h = model.InitHiddenstate(batch_size)
                model.train()
                for windows, labels in trainloader:

                    h = tuple([e.data for e in h])
                    #Training pass
                    optimizer.zero_grad()
                    out, h = model(windows, h)
                    loss = criterion(out.squeeze(), labels)
                    loss.backward()
                    nn.utils.clip_grad_norm_(model.parameters(), clip)
                    optimizer.step()
                    train_loss += loss.item() * windows.size(0)


                #####################
                ## Validation loop ##
                #####################
                h = model.InitHiddenstate(batch_size)
                model.eval()
                for windows, labels in validloader:
                    
                    h = tuple([each.data for each in h])

                    #Validation pass
                    out, h = model(windows, h)
                    loss = criterion(out, labels)
                    valid_loss += loss.item()*windows.size(0)

                    #Averages losses
                    train_loss = train_loss/len(trainloader.sampler)
                    valid_loss = valid_loss/len(validloader.sampler)
                    
                    train_list.append(train_loss)
                    valid_list.append(valid_loss)

                    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
                    epoch, train_loss, valid_loss))
                    
                    # save model if validation loss has decreased
                    if valid_loss <= valid_loss_min:
                        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
                        valid_loss_min, valid_loss))

                        torch.save(model.state_dict(), "pytorch/trained_models/"+participant+"_"+modality+".pt")
                        valid_loss_min = valid_loss

        ################
        ### EEG LOOP ###
        ################
        elif modality == "EEG": #If the network includes an LSTM layer (holds for PPG & GSR) 
            for epoch in range(1, epochs+1):
                    
                train_loss = 0.0
                valid_loss = 0.0

                ###################
                ###Training loop###
                ###################
                model.train()
                for windows, labels in trainloader:

                    # move tensors to GPU if CUDA is available
                    if train_on_gpu:
                        windows, labels = windows.cuda(), labels.cuda()

                    #Training pass
                    optimizer.zero_grad()
                    out = model(windows)
                    loss = criterion(out.squeeze(), labels)
                    loss.backward()
                    optimizer.step()
                    train_loss += loss.item() * windows.size(0)


                ###################
                ##Validation loop##
                ###################
                model.eval()
                diff = torch.cuda.FloatTensor()
                
                predictions = torch.cuda.FloatTensor()
                labelss = torch.cuda.FloatTensor()

                for windows, labels in validloader:

                  if train_on_gpu:
                      windows, labels = windows.cuda(), labels.cuda()
                  #Test pass    
                  out = model(windows)
                  loss = criterion(out.squeeze(), labels)
                  valid_loss += loss.item()*windows.size(0)

                  foo = (out.squeeze() - labels)
                  diff = torch.cat([diff,foo])

                  predictions = torch.cat([predictions, out])
                  labelss = torch.cat([labelss, labels])

                average_miss = sum(abs(diff))/len(validloader.sampler)
                
                """
                # save model if validation loss has decreased
                if valid_loss <= valid_loss_min:
                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
                    valid_loss_min, valid_loss))
                  
                    torch.save(model.state_dict(), participant+"_"+modality+".pt")
                    valid_loss_min = valid_loss
                """
                accuracy = average_miss
                
                trial.report(accuracy, epoch)

                # Handle pruning based on the intermediate value.
                if trial.should_prune():
                    raise optuna.exceptions.TrialPruned()

        return accuracy

"""# Main"""

participants = ["bci10", "bci12", "bci13", "bci17", "bci20", "bci21", "bci22",
                "bci23", "bci24", "bci26", "bci27", "bci28", "bci29", "bci30", 
                "bci31", "bci32", "bci33", "bci34", "bci35", "bci36", "bci37", 
                "bci38", "bci39", "bci40", "bci41", "bci42", "bci43", "bci44"]

epochs = 50
trainortest = "train"
np.random.seed(3791)
torch.manual_seed(3791)
modality = "EEG"
batch_size = 10

train_on_gpu = torch.cuda.is_available()

from google.colab import drive
drive.mount('/content/drive')


if __name__ == "__main__":
    
    study_name = "_hpo"
    folder = "HPO/search"

    study = optuna.create_study(direction="minimize", storage="sqlite:///example.db")
    study.optimize(objective, n_trials=25)

    pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]
    complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]

    print("Study statistics: ")
    print("  Number of finished trials: ", len(study.trials))
    print("  Number of pruned trials: ", len(pruned_trials))
    print("  Number of complete trials: ", len(complete_trials))

    print("Best trial:")
    trial = study.best_trial

    print("  Value: ", trial.value)

    print("  Params: ")
    for key, value in trial.params.items():
        print("    {}: {}".format(key, value))

    optuna.visualization.plot_param_importances(study)

#optuna.load_study(/content/example.db)